---
title: "Casos Del Mundo Real - Web Scraping"
author: "Fabricio Lennart"
date: "2025-09-16"
format:
  html:
    theme: darkly
    toc: true
    toc-location: left
    toc-depth: 3
    code-fold: true
    code-summary: "Mostrar c√≥digo"
jupyter: python3
execute:
  echo: true
  warning: false
  message: false
---
# üöÄ ¬°Manos a la Obra!

---

### Preparandonos para el Mundo Real

En esta clase, vamos a sumergirnos de lleno en un escenario que ver√°n muy a menudo en el mundo del **web scraping**. Imaginenemos esto: una tienda online nos contrat√≥. Su objetivo es simple: quieren saber qu√© hace su competencia. Nuestra tarea ser√° **"raspar"** los productos en **oferta** de uno de sus rivales para que puedan comparar precios, caracter√≠sticas y mejorar sus propias estrategias.

---

### üåê La Web de Prueba: 

Para este reto, vamos a usar una p√°gina web dise√±ada espec√≠ficamente para practicar web scraping:

[https://webscraper.io/test-sites/e-commerce/allinone](https://webscraper.io/test-sites/e-commerce/allinone)

Aqu√≠ es donde nuestro c√≥digo har√° su magia para encontrar y extraer los datos de los productos.

---


```{python}
from bs4 import BeautifulSoup
import requests as rq

url = "https://webscraper.io/test-sites/e-commerce/allinone"
respuesta = rq.get(url)
soup = BeautifulSoup(respuesta.text, "html.parser")

productos_html = soup.select("div.col-md-4.col-xl-4.col-lg-4")

productos = []
for producto in productos_html:
    # Extrae el nombre COMPLETO del atributo 'title' del enlace <a>
    nombre_completo = producto.select_one(".title")["title"] 
    precio = producto.select_one(".price span").get_text(strip=True)
    productos.append({"Product": nombre_completo, "Price": precio})
    print(f"{nombre_completo} - {precio}")

print(f"Se encontraron {len(productos)} productos")
```


---

# üíæ Guardando Nuestros Datos: CSV


```{python}

import csv
from pathlib import Path

# Definir rutas
project_base = Path("/persistent/home/fabri/proyectos/WebScrapingToolkit")
output_dir = project_base / "Clase04" / "recursos"
output_file = output_dir / "productos.csv"

# Crear directorio si no existe
output_dir.mkdir(parents=True, exist_ok=True)

# Guardar CSV 
with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
    writer = csv.DictWriter(csvfile, fieldnames=["Product", "Price"])
    writer.writeheader()
    writer.writerows(productos)

# Mostrar ubicaci√≥n (ruta relativa al proyecto)
print(f"‚úì CSV guardado en: {output_file.relative_to(project_base)}")

```

---

# üíæ Guardando Nuestros Datos: JSON

```{python}
import json
from pathlib import Path

# Definir rutas
project_base = Path("/persistent/home/fabri/proyectos/WebScrapingToolkit")
output_dir = project_base / "Clase04" / "recursos"
output_file = output_dir / "products.json"  # Nombre del archivo JSON

# Crear directorio si no existe
output_dir.mkdir(parents=True, exist_ok=True)

# Guardar datos como JSON
with open(output_file, "w", encoding='utf-8') as f:  # Corregido: solo la ruta
    json.dump(productos, f, indent=4, ensure_ascii=False)  # ensure_ascii=False para caracteres especiales

# Mostrar ubicaci√≥n (ruta relativa al proyecto)
print(f"‚úì JSON guardado en: {output_file.relative_to(project_base)}")
```


### El Paso Final y M√°s Importante: Elegir D√≥nde Guardar tus Datos

Una vez que logramos extraer esa informaci√≥n valiosa de la web, viene una pregunta crucial: **¬ød√≥nde y c√≥mo la guardamos?** La forma en que almacenamos nuestros datos no es un detalle menor; depende totalmente de **qui√©n va a usar esa informaci√≥n y para qu√©**.

Pensemoslo as√≠:

* Si los datos los va a analizar un equipo de marketing en una hoja de c√°lculo, formatos como **CSV**, **XLS** o **XLSX** son perfectos porque son compatibles con programas como Excel.
* Pero si la informaci√≥n va a ir directamente a una base de datos **NoSQL** (como MongoDB) o si necesitamos intercambiarla con una **API**, entonces el formato **JSON** es la mejor opci√≥n. Es super flexible y maneja muy bien las estructuras de datos complejas.

Aprender a guardar tus datos en el formato correcto es lo que realmente hace que todo el esfuerzo del *scraping* valga la pena, transformando la informaci√≥n cruda en un recurso √∫til y listo para usar.

---

